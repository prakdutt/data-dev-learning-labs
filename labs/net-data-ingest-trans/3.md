# Step 3: View Data in Visualization Tool

Select file you want to visualize, then click Visualize button.

![alt tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/net-data-ingest-trans/assets/images/visualize0.png?raw=true)

Visualize View:
![alt tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/net-data-ingest-trans/assets/images/visualize2.png?raw=true)


Sample Code: Transform a network data file and store transformed file to HDFS.

Once all the above mentioned steps has been done, a new project with class name will be created. We can start coding for getting the csv file of network data from HDFS, perform the transform, and then store the transformed data to HDFS.
```java
//Import the below libraries which are used to run the code.
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.FileUtil;
import org.apache.hadoop.fs.Path;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.FlatMapFunction;

import java.util.Arrays;
//Class: TransformData. A class that transform the network data
public class TransformData {

    public static void main(String [] args) throws Exception {


  //three parameters here
        String filePath= args[0];
        String outputPath= args[1];
        String distFilePath= args[2];

        final String seperatorField = ",";

        //System.out.println("master address:"+master);
        System.out.println("input file path:"+filePath);
// here we are giving application name as “File transform”
        SparkConf conf = new SparkConf().setAppName("File transform");
        JavaSparkContext sc = new JavaSparkContext(conf);

        JavaRDD<String> textFile = sc.textFile(filePath);

        JavaRDD<String> cols = textFile.flatMap(new FlatMapFunction<String, String>() {
            public Iterable<String> call(String s) {
                String [] splits = s.split(seperatorField) ;

                String extractedCols = "";

                for(int i= 1 ;i<= 6 ;i ++ ){
                    extractedCols += splits[i ] + seperatorField ;
                }

                if(extractedCols.endsWith(seperatorField)){
                    extractedCols = extractedCols.substring(0,extractedCols.length() - 1)  ;
                }

                return Arrays.asList(extractedCols);
            }
        });


       cols.coalesce(1).saveAsTextFile(outputPath);

       boolean bool = getMergeInHdfs(outputPath,distFilePath);
        System.out.println("merge success:"+bool);



    }

    public static boolean getMergeInHdfs(String src, String dest) throws IllegalArgumentException, Exception {

        Configuration config = new Configuration();
        FileSystem fs = FileSystem.get(config);
        Path srcPath = new Path(src);
        Path dstPath = new Path(dest);

        // Check if the path already exists
        if (!(fs.exists(srcPath))) {
            System.out.println("Path " + src + " does not exists!");
            return false;
        }

        if ((fs.exists(dstPath))) {
            System.out.println("File Path " + dest + " exists!");
            return false;
        }

        return FileUtil.copyMerge(fs, srcPath, fs, dstPath, true, config, null);
    }
}
```
