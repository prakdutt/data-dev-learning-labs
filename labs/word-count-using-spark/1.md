# <center>Word Count Using SPARK</center>

## Lab Overview

WordCount is a classic example in learning Spark, as Hello World in learning any programming language. This Learning Lab introduces how to use IDE in DLP to create a WordCount program and how to execute it to get the intended result. After completing this lab, the user is encouraged to try on their own for different variations of a data file. 


## Lab Objectives

* Learn how to get data from a file.
* Learn how to use some RDD operations on data.
* Learn how to write data to a new file with file creation in HDFS.


## Prerequisites

* Knowledge on Hadoop to store the data.
* Basic knowledge of how spark works.
* Use Chrome OS


## Step 1: Explore DLP platform

DLP website URL ([link](https://developer.cisco.com/))

[http://10.10.10.74/DDPDevUI/demo/#/ddp/login]

1)	After logging on DLP, click on Development Hub on the left column.<br>
2)	Choose a workspace; here we use a pre-defined Scala environment, named wksp-scala.<br>
3)	Click on the Launch button to open IDE workspace.<br>

![alt-tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/steps1.PNG?raw=true)

## Step 2: Explore IDE Workspace

1)	In IDE workspace, code files are listed on the left column.<br>
![Figure](/posts/files/word-count-using-spark/assets/images/step2-1.jpg)

2）There is a file named build.sbt, which contains the environment for all the jobs to run.<br>
![Figure](/posts/files/word-count-using-spark/assets/images/step2-2.jpg)<br>

The build.sbt file has the following content:<br>

 ```
 libraryDependencies += "org.apache.spark" %% "spark-core" % "1.3.0"
 libraryDependencies += "org.apache.spark" %% "spark-sql" % "1.3.0"
 libraryDependencies += "org.apache.spark" %% "spark-hive" % "1.3.0"
 libraryDependencies += "org.apache.spark" %% "spark-streaming" % "1.3.0"
 name := "hello" 
 version := "1.0"
 scalaVersion := "2.11.7"
 ```

## Step 3: Coding Exercise

In this coding exercise, we provide you a data file called wordcountinputfile.<br>
This program will count the number of words in the file by identifying spaces between words. 

```
//Import libraries which are needed to run the program. 
import org.apache.spark.{SparkContext, SparkConf}
object WordCount
{
  def main(args: Array[String]) {
    val inputFile = args(0)
    val outputFile = args(1)
    val conf = new SparkConf().setAppName("wordCount")
    // Create a Scala Spark Context.
    val sc = new SparkContext(conf)
    // Load our input data.
    val input = sc.textFile(inputFile)
    // Split up into words.
    val words = input.flatMap(line => line.split(" "))
    // Transform into word and count.
    val counts = words.map(word => (word, 1)).reduceByKey{_ + _}
    // Save the word count back out to a text file, causing evaluation.
    counts.saveAsTextFile(outputFile)
  }
}
```

### Two approaches to run the program

 <b><font color='red'>First Approach</font></b><br>

l) Click on the Run and select the Terminal option.<br>
2) Use the command cd worldcount && sbt package to bulid the sbt environment.<br>
3) Use the command **HADOOP\_USER\_NAME=devnet spark-submit --master yarn --deploy-mode cluster –class WordCount /projects/worldcount/target/scala-2.11/hello_2.11-1.0.jar parameter1 parameter2** to run the job.<br>
<b>Parameter1:</b> The input path to get the data.<br>
<b>Parameter2:</b> The output path to store the transformed data.<br>

<b><font color='red'>Second Approach</font></b><br>
1)	Click on CMI on top of IDE screen and choose Edit Commands, a user can see the shell command here.<br>

![Figure](/posts/files/word-count-using-spark/assets/images/step3-1.jpg)

2)	Edit WordCountRun.sh to change parameters.<br>


![Figure](/posts/files/word-count-using-spark/assets/images/step3-2.jpg)

Choose a Command under Edit Commands, then click the blue button 
![Figure](/posts/files/word-count-using-spark/assets/images/step3-3.jpg) to run, as in the Terminal.<br>

    First run the package command.
    Then, run the WordCountRun command.

![Figure](/posts/files/word-count-using-spark/assets/images/step3-4.jpg)


## Step 4: See result from HDFS

In the terminal, use command hadoop fs –cat <font color='red'>hdfs://172.16.1.11:8020/tmp/spark/output/output30/part-00*</font> to see the result.<br>

![Figure](/posts/files/word-count-using-spark/assets/images/step4.jpg)


##### An Example:

**Input:**<br>
Welcome to DDP. This is your first program in DDP. //which is present in input file.<br>

**Output:** <br>

```
Welcome	1
to	    1
DDP. 	2
This 	1
Is 		1
Your 	1
First 	1
Program 1
In 		1
```

## Things to Try


* Try with numeric data type
* Try with case sensitive data as well.

Completing this coding exercise, we have learned how to count the number of words in an input file using Spark Batch Processing. <br>
There are some more examples and exercise are available in the below mentioned link. This is for your reference.
[http://spark.apache.org/docs/latest/programming-guide.html](http://spark.apache.org/docs/latest/programming-guide.html).




