# <center>Word Count Using SPARK</center>

## Lab Overview

WordCount is a classic example in learning Spark, as Hello World in learning any programming language. This Learning Lab introduces how to use IDE in DLP to create a WordCount program and how to execute it to get the intended result. After completing this lab, the user is encouraged to try on their own for different variations of a data file. 


## Lab Objectives

* Learn how to get data from a file.
* Learn how to use some RDD operations on data.
* Learn how to write data to a new file with file creation in HDFS.


## Prerequisites

* Knowledge on Hadoop to store the data.
* Basic knowledge of how spark works.
* Use Chrome OS


## Step 1: Explore DLP platform

DLP website URL ([link](https://developer.cisco.com/))

[http://10.10.10.74/DDPDevUI/demo/#/ddp/login]

1)	After logging on DLP, click on Development Hub on the left column.<br>
2)	Choose a workspace; here we use a pre-defined Scala environment, named "wksp-scala".<br>
3)	Click on the Launch button to open IDE workspace.<br>

![alt-tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/steps1.PNG?raw=true)

## Step 2: Explore IDE Workspace

1)	Below two sections will help to explore the IDE for Word Count project. In the IDE, files are listed in Left Panel.
![alt-tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/step2.png?raw=true)

2ï¼‰There is a file named build.sbt, which contains the environment for all the jobs to run.<br>
![alt-tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/step4.PNG?raw=true)

The build.sbt file has the following content:<br>
 ```
 libraryDependencies += "org.apache.spark" %% "spark-core" % "1.3.0"
 libraryDependencies += "org.apache.spark" %% "spark-sql" % "1.3.0"
 libraryDependencies += "org.apache.spark" %% "spark-hive" % "1.3.0"
 libraryDependencies += "org.apache.spark" %% "spark-streaming" % "1.3.0"
 name := "hello" 
 version := "1.0"
 scalaVersion := "2.11.7"
 ```

## Step 3: Coding Exercise

In this coding exercise, we provide you a data file called wordcountinputfile.<br>
This program will count the number of words in the file by identifying spaces between words. 

```
//Import libraries which are needed to run the program. 
import org.apache.spark.{SparkContext, SparkConf}
object WordCount
{
  def main(args: Array[String]) {
    val inputFile = args(0)
    val outputFile = args(1)
    val conf = new SparkConf().setAppName("wordCount")
    // Create a Scala Spark Context.
    val sc = new SparkContext(conf)
    // Load our input data.
    val input = sc.textFile(inputFile)
    // Split up into words.
    val words = input.flatMap(line => line.split(" "))
    // Transform into word and count.
    val counts = words.map(word => (word, 1)).reduceByKey{_ + _}
    // Save the word count back out to a text file, causing evaluation.
    counts.saveAsTextFile(outputFile)
  }
}
```
N.B: Use WordCount.scala to view the above code. 

# Step 4 :  Run the Program

1) Select the WordCountRun.sh file.
![alt-tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/Step7.PNG?raw=true)

There are two parameters. 
a) Input Parameter: Here input file from hdfs path is "wordcountinputfile1.txt"
b) Output Parameter: Here output file which would be saved in hdfs is "output47"

2)	Click on CMI on top of IDE screen and choose Edit Commands, a user can see the shell command here. 
On CMI, Select "Edit Commands...". It will open a "Commands" window. From this window, select "WordCountRun" and it will select WordCountRun.sh file.

![alt-tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/step5.PNG?raw=true)

![alt-tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/Step6.PNG?raw=true)

3) Select the WordCountRun.sh file.
![alt-tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/Step7.PNG?raw=true)

4)	Choose a Command under Edit Commands, then click the blue button   to run, as in the Terminal.
First run the package command.
Then, run the WordCountRun command.
![alt-tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/Step8.PNG?raw=true)

# Step 5: See Result in HDFS
1) Edit WordCountResult.sh to change parameters as described below:
![alt-tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/Step9.png?raw=true)

The parameter is the output path to store the transformed data and the /part-*
e.g. hdfs://172.16.1.11:8020/tmp/spark/output/output47/part-00*

2)  Choose the Command WordCountResult under Edit Commands, then click the blue button to run.
![alt-tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/step4.jpg?raw=true)

An Example:
Input: Welcome to DDP. This is your first program in DDP. //which is present in input file.

Output: 
Welcome 1<br>
to      1<br>
DDP. 	2<br>
This 	1<br>
Is 	1<br>
Your 	1<br>
First 	1<br>
Program 1<br>
In 	1<br>

Things to Try
* Try with numeric data type
* Try with case sensitive data as well.
<br>
Completing this coding exercise, we have learned how to count the number of words in an input file using Spark Batch Processing. 

<br>
There are some more examples and exercise are available in the below mentioned link. This is for your reference.
http://spark.apache.org/docs/latest/programming-guide.html.
